# Alzheimers_Detection_Model
 
### Introduction
* This repository is about training an Alzheimer's disease detection model. The data is obtined from Dementia Bank, which is an opensource corpus that include audio files for research. The intention of developing this model is to combat the aging society. According to the statistics, 3.9% of people who aged 60+ years have Alzheimer's Disease. By 2050, the world population of people over age 65 will rise to 16%. This means more people will become susceptable to Alzheimer's Disease. In traditional medical diagnosis, the Alzheimer's disease is determined by the Mini-Mental State Exam (MMSE). MMSE is a standard procedure to diagnose patient. The higher the score, the more likely the person would be identified as having Alzheimer's disease. However, MMSE is a behavioural measurement and could be influenced by many confounding factors. Therefore, having a detection model to further support the diagnosis process is very important.
* Lastly, I am very interested in the data science application in life-science. I'd learned a lot from exploring this topic!

### Overview
* The Alzheimer's disease detection model is trained on the acoustic features extracted from audio files. One major advantage of using acoustic features is because it eliminates the concern of different languages. Therefore, the model should be able to make accurate prediction regardless of where the patients came from. I'd experimented with different acoustic feature packages using the openSmile software. openSmile includes many different feature extraction packages on audio signals for speech classification. Some of the well-known feature sets are the InterSpeech and EmoLarge. However, I obtained the best model using the MFCC feature sets, which is not included in the openSMILE package. The model is trained by AutoML tool and achieves 75% accuracy on test set.

### ETL
1. Use Praat software to extract sentences spoken by the patient and load it to the CLAN software
2. Download the audio directly from the CLAN software
3. Organize each patient's audio files to their respective classes: Control, Dementia, and Mild Cognitive Impairment(MCI)

### Preprocessing audio
1. Combine the each patient's audio files to obtain a complete audio file
2. Denoise the audio file
3. Calculate the average decibels relative to full scale(dBFS) for the entire dataset 
4. Manually remove some patient out of the dataset either due to loud background noise or low speaking volumne.
5. Segment each patient's complete audio file into multiple 4 seconds audios
6. Remove silence segment from the 4 seconds audio files
7. Combine the 4 seconds audio files from each patient together again to obtain the complete audio file without silence segment
8. Now, we have two different datasets: 4 seconds segmentwise audio data and complete audio data
9. Normalize both datasets by the dBFS values

### Obtaining acoustic features
1. extract the Mel-frequency cepstral coefficients (MFCCs) from the audio. In my work, the dimension should be around 150 for each audio
2. Perform feature selection to obtain high quality features
3. Standardlize the values for the selected features


### Files in this Repo

##### config 
* Contains parameters to be specified to run the inference.

##### model
* Contains the trained models using AutoML

##### MFCC
* Contains the MFCC acoustic features used for training in csv format.

##### Postman
* Contains the processed information that is fed into AutoML for training the model.

##### src
* Contains useful function for pre-processing the audios and extracting features.

##### results
* Contains plot generated by the inference process

##### notebook
* contains jupyter notebooks to visualize the inference results

##### evaluate_person.py
* The script to evaluate the model trained on complete audio.

##### evaluate_segment.py
* The script to evaluate the model trained on 4 seconds segmented audio.

##### requirements.txt
* packages necessary to run the script.

### How to run this repo:
1. please type this command `pip install -r requirements.txt` to install necessary packages to run this program
2. You can re-arrange the data path in the .json file inside the `config` folder
3. Run either `evaluate_person.py` or `evaluate_segment.py` to get my inference results!
4. If you would like to train your own model with different acoustic feature sets, please visit the openSmile github repo listed in the reference to obtain other features

### References
* openSmile repo: https://github.com/Zhangtingyuxuan/AcousticFeatureExtraction
